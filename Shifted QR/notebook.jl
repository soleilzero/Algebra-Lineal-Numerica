### A Pluto.jl notebook ###
# v0.20.3

using Markdown
using InteractiveUtils

# ‚ïî‚ïê‚ï° c478a7cc-42b0-11f0-1c45-919167ce835a
md"
# The shifted QR iteration
"

# ‚ïî‚ïê‚ï° 17dbbafe-fe46-4001-a1a0-5636395e89d8
md"
Plan:
1. Introducci√≥n
 * Objetivos
 * Nociones
 * Motivaci√≥n
2. Teor√≠a
 * Recap of eigenvalue problems.
 * The basic QR algorithm.
 * The shifted QR method (Wilkinson shift, etc.).
 * Convergence behavior and complexity.
3. Implementaci√≥n

 * Code for the unshifted QR algorithm (for comparison).

 * Code for the shifted QR algorithm.

 * Optional: Use of Hessenberg reduction to optimize performance.
4. Experimentos
5. An√°lisis
"

# ‚ïî‚ïê‚ï° 5a04ad92-ed1f-4674-858a-f36de9a335a4
md"
## Introducci√≥n
### Objetivos
* Explicar claramente el m√©todo de descomposici√≥n QR con desplazamiento, haciendo √©nfasis en los aspectos te√≥ricos y computacionales relevantes,

* implementar el algoritmo de Shifted QR en Julia,

* realizar experimentos num√©ricos bien dise√±ados, que permitan ilustrar el comportamiento del m√©todo bajo diferentes condiciones (tama√±o de matrices, precisi√≥n, n√∫mero de iteraciones, etc.),

* analizar y discutir los resultados, con observaciones fundamentadas tanto en la teor√≠a como en los datos obtenidos.

* resumir los hallazgos m√°s relevantes y, plantear preguntas o ideas para trabajo futuro.

### Motivaci√≥n
### ?
Aceleraci√≥n de convergencia
Eficiencia en algoritmos modernos (desplazamientos‚Äîexpl√≠citos o impl√≠citos‚Äîincrementan el aislamiento de subbloques triangulares de la matriz, reduciendo r√°pidamente la parte activa del problema)
"

# ‚ïî‚ïê‚ï° 06a284b8-ab2d-4b12-9153-5f8e88485e78
md"
## Teor√≠a 

### M√©todo QR b√°sico y limitaciones

#### Esquema del algoritmo

El m√©todo QR es una iteraci√≥n matricial basada en la descomposici√≥n QR:

Dado $A_0=A$, se realiza en cada paso:

* Descomposici√≥n QR de $A_k$: $A_k=Q_kR_k \text{}$

* C√°lculo de $A_{k+1}$: $A_{k+1}=R_kQ_k$

Notemos que cada iteraci√≥n produce una matriz similar a la anterior:
$A_{k+1}=Q_k^TA_kQ_k$


Por tanto, todas las $A_k$ son similares a la original $A$, y tienen los mismos autovalores. 

Bajo ciertas condiciones (e.g., $A$ sim√©trica), las $A_k$ convergen a una matriz diagonal cuya diagonal contiene los autovalores de $A$.

#### Convergencia

Este tema es tratado en la secci√≥n 7.3.3 del libro.

Sea $A \in \mathbb{C}^{n \times n}$ una matriz diagonalizable con valores propios ordenados por magnitud:

$|\lambda_1| > |\lambda_2| > \cdots > |\lambda_n|.$

Supongamos que $A$ tiene una descomposici√≥n de Schur:

$A = Q_0 T_0 Q_0^H,$

donde $Q_0$ es unitario y $T_0$ es triangular superior con los valores propios de $A$ en la diagonal.

El algoritmo QR sin shift genera una secuencia de matrices $\{A^{(k)}\}$ mediante:

$A^{(k)} = Q_k R_k, \quad A^{(k+1)} = R_k Q_k,$

con lo cual:

$A^{(k+1)} = Q_k^{-1} A^{(k)} Q_k.$

**Teorema (convergencia de subespacios):**

Sea $e_1 \in \mathbb{C}^n$ el primer vector can√≥nico. Si $A$ tiene vectores propios 
$\{x_i\}$, con $A x_i = \lambda_i x_i$, entonces:

$\lim_{k \to \infty} A^{(k)} = T = \text{matriz triangular superior similar a } A,$

y adem√°s, la subespacio generado por $A^{(k)} e_1$ converge al **autovector dominante** $x_1$, con raz√≥n:

$\left| \frac{\lambda_2}{\lambda_1} \right|^k.$

M√°s espec√≠ficamente, los elementos subdiagonales de $A^{(k)}$ se aten√∫an con raz√≥n lineal:

$|(A^{(k)})_{i+1, i}| = O\left(\left|\frac{\lambda_{i+1}}{\lambda_i}\right|^k\right).$

Es decir que el algoritmo QR sin shift **converge linealmente**, y su velocidad depende del cociente entre valores propios consecutivos. La convergencia puede ser muy lenta si los valores propios est√°n cercanos en magnitud.

##### Estabilidad del m√©todo QR b√°sico

El m√©todo QR se basa en transformaciones ortogonales (o unitarias, en el caso complejo), las cuales son num√©ricamente estables porque **preservan la norma 2** y no amplifican los errores de redondeo. En cada paso, se realiza una transformaci√≥n de similaridad de la forma:

$A_{k+1} = Q_k^\top A_k Q_k$

Estas transformaciones no degradan la condici√≥n del problema ni introducen inestabilidad inherente. Por esta raz√≥n, el m√©todo QR es considerado num√©ricamente estable: los autovalores obtenidos son aproximaciones fiables de los autovalores exactos de la matriz original, dentro de los l√≠mites del error de redondeo.

---

#### Precisi√≥n del m√©todo QR b√°sico

La precisi√≥n en los autovalores calculados depende principalmente de:

* La separaci√≥n entre los autovalores (problemas con autovalores cercanos presentan mayor sensibilidad).
* La acumulaci√≥n de errores en la descomposici√≥n QR (influenciada por la t√©cnica empleada: Householder, Givens, MGS, etc.).

Seg√∫n el an√°lisis de Golub y Van Loan, el m√©todo QR aplicado a una matriz sim√©trica produce autovalores $\hat{\lambda}_i$ tales que:

$|\hat{\lambda}_i - \lambda_i| \approx u \|A\|_2$

donde $u$ es la unidad de redondeo (por ejemplo, $u \approx 10^{-16}$ en doble precisi√≥n), y $\|A\|_2$ es la norma espectral de la matriz.
"

# ‚ïî‚ïê‚ï° 891fb287-b56b-46e1-bd7c-93eb7ca4c9ad
md"""
## M√©todo QR con shift
### Motivaci√≥n

Dado que la velocidad de convergencia del m√©todo QR b√°sico es el cociente entre los dos primeros autovalores, este m√©todo puede presentar **convergencia lenta** cuando los autovalores de la matriz est√°n cercanos entre s√≠. Esta lentitud se traduce en una mayor cantidad de iteraciones y, por tanto, un mayor costo computacional.

### Definici√≥n

Sea $A \in \mathbb{R}^{n \times n}$ una matriz real. El **m√©todo QR con desplazamiento** es una variante del algoritmo QR cl√°sico que acelera la convergencia mediante la incorporaci√≥n de un escalar $\mu_k \in \mathbb{R}$ en cada iteraci√≥n.

##### Tipos
Est√°tico vs din√°mico

#### Esquema general de la iteraci√≥n

Dado $A_0 := A$, para cada $k \geq 0$ realizamos:

1. **Aplicar el shift**:

   $A_k - \mu_k I = Q_k R_k$

   donde $Q_k$ es ortogonal y $R_k$ es triangular superior.

2. **Actualizar la matriz**:

   $A_{k+1} = R_k Q_k + \mu_k I$

##### Justificaci√≥n algebraica del shift est√°tico
Esta operaci√≥n puede reescribirse como:
$A_{k+1} = Q_k^\top A_k Q_k$,
debido a que:

$Q_k^TA_kQ_k$

$=Q_k^T (Q_k R_k+ \mu I) Q_k$

$=Q_k^T Q_k R_k Q_k + \mu Q_k^T Q_k$

$=R_k Q_k +\mu I$

$= A_k$

Esto implica que $A_{k+1} \sim A_k \sim A$: la matriz resultante es similar a la anterior, y por tanto **preserva los autovalores**. En otras palabras, el m√©todo QR con shift genera una sucesi√≥n de matrices similares entre s√≠.

#### Forma de Hessenberg
Las matrices de Hessenberg
...
#### Elecci√≥n del desplazamiento
#### Variando el desplazamiento
Escogiendo $h_{nn}$ cada vez.

#### Convergencia
La convergencia es lineal, y el cociente espectral afecta el ritmo de convergencia:
$\bigg|\frac{\lambda_{i+1}‚àíu}{\lambda_i‚àíu}\bigg|^k.$ (Analizado en la secci√≥n 7.5.2)


Un shift est√°tico no induce deflaci√≥n r√°pida, a diferencia del shift din√°mico basado en Rayleigh o Wilkinson. 
"""

# ‚ïî‚ïê‚ï° e4c54b68-e57e-4dc4-ba2e-bdc055ec67ba
md"
### Versi√≥n cambiando valor
#### Convergencia 
Con mucho gusto. A continuaci√≥n presento una redacci√≥n formal para la subsecci√≥n **Convergencia del m√©todo QR con desplazamiento**, basada en *Matrix Computations* (Golub & Van Loan, 4ta ed., secci√≥n 7.5.2), en un estilo acorde al notebook:

---

### üìà Convergencia del m√©todo QR con desplazamiento

El m√©todo QR sin desplazamiento converge en general **lentamente**, especialmente cuando los autovalores est√°n pr√≥ximos entre s√≠ o no bien separados en magnitud. La introducci√≥n de un desplazamiento $\mu_k$ en cada iteraci√≥n no s√≥lo acelera la convergencia, sino que **altera el comportamiento espectral de forma favorable**.

#### üß† Intuici√≥n espectral

El desplazamiento tiene el efecto de acercar un autovalor particular al origen del espectro de la matriz desplazada $A_k - \mu_k I$. Como el algoritmo QR aplicado a una matriz desplazada se comporta an√°logamente al **m√©todo de la potencia inversa**, este desplazamiento **atrae la convergencia hacia el autovalor m√°s cercano a $\mu_k$**.

Despu√©s de reconstruir la matriz con $A_{k+1} = R_k Q_k + \mu_k I$, ese autovalor aparece m√°s claramente en la forma (cuasi)triangular de $A_{k+1}$.

---

### üß™ Resultados te√≥ricos

#### üîπ Caso sim√©trico (autovalores reales y ortogonal diagonalizaci√≥n)

Si $A \in \mathbb{R}^{n \times n}$ es sim√©trica, entonces el m√©todo QR con desplazamiento converge **cuadr√°ticamente** hacia una matriz diagonal:

$$\lim_{k \to \infty} A_k = \operatorname{diag}(\lambda_1, \dots, \lambda_n)$$

y los vectores $Q_0 Q_1 \cdots Q_k$ convergen a una matriz ortogonal cuyos vectores columna son autovectores de $A$.

> **Con desplazamiento**, la velocidad de convergencia se ve aumentada en comparaci√≥n al caso sin shift. En particular, se muestra (ver GVL, ¬ß7.5.2) que:
>
> * Si $A$ es sim√©trica tridiagonal,
> * y se usa un desplazamiento de Wilkinson (definido m√°s adelante),
>
> entonces los elementos subdiagonales convergen a cero a **una velocidad cuadr√°tica**, i.e., $|a_{n,n-1}^{(k)}| = O(|a_{n,n-1}^{(k-1)}|^2)$.

Esta mejora se debe a que el shift de Wilkinson se elige estrat√©gicamente para aproximar un autovalor dominante en la esquina inferior de la matriz, lo que acelera la separaci√≥n espectral de ese valor.

---

### ‚è± Comparaci√≥n entre QR sin shift y con shift

| Caracter√≠stica            | QR sin desplazamiento   | QR con desplazamiento            |
| ------------------------- | ----------------------- | -------------------------------- |
| Velocidad de convergencia | Lineal                  | Cuadr√°tica (en casos favorables) |
| Estructura aprovechada    | Ninguna                 | Tridiagonal (ideal), simetr√≠a    |
| Posibilidad de deflaci√≥n  | Dif√≠cil                 | Natural (cuando subdiagonal ‚Üí 0) |
| Costo por iteraci√≥n       | $O(n^2)$ con Hessenberg | Igual, pero menos iteraciones    |
| Pr√°ctico para uso real    | No                      | S√≠                               |

---

### üìå Comentario adicional: efecto en matrices no sim√©tricas

En matrices generales (no sim√©tricas), el m√©todo QR con shift no necesariamente converge a una forma diagonal, sino a la **forma de Schur** (triangular superior). No obstante, el desplazamiento sigue siendo √∫til: acelera la aparici√≥n de valores dominantes en la diagonal, y favorece la **deflaci√≥n** local.


#### Complejidad




#### üíª Eficiencia computacional

* Para matrices generales, el costo por iteraci√≥n es $O(n^3)$.
* Sin embargo, si $A_k$ se mantiene en **forma de Hessenberg** (lo cual es usual en implementaciones pr√°cticas), el costo se reduce a $O(n^2)$ por iteraci√≥n, gracias a la estructura esparsa.

Este algoritmo constituye la base de la versi√≥n moderna del QR para c√≥mputo de autovalores y es utilizado en bibliotecas num√©ricas como LAPACK.

"

# ‚ïî‚ïê‚ï° 6db839bb-ea7b-486d-9ef0-19d476273b85
md"
# Notas

#### üìå Observaciones importantes

* Si $\mu_k = 0$ para todo $k$, se recupera el m√©todo QR sin desplazamiento.
* A diferencia del QR cl√°sico, el uso del shift permite orientar la iteraci√≥n hacia una parte espec√≠fica del espectro.
* Las iteraciones est√°n dise√±adas para hacer que ciertos elementos subdiagonales de $A_k$ se reduzcan r√°pidamente a cero, facilitando la **deflaci√≥n**.

#### üí° Justificaci√≥n algebraica

Sea $A_k = Q_k R_k + \mu_k I$, con

$A_k - \mu_k I = Q_k R_k \quad \Rightarrow \quad A_k = Q_k R_k + \mu_k I$

Entonces:

$A_{k+1} = R_k Q_k + \mu_k I = Q_k^\top A_k Q_k$

Esto implica que cada paso es una transformaci√≥n ortogonal de similaridad: la matriz $A_k$ es transformada por conjugaci√≥n ortogonal con $Q_k$. Por lo tanto, todos los $A_k$ son ortogonalmente similares entre s√≠, y sus autovalores (en exactitud matem√°tica) son id√©nticos.



#### üß† Interpretaci√≥n espectral

En el contexto de autovalores, restar $\mu_k I$ a $A_k$ equivale a considerar los autovalores $\lambda_i - \mu_k$. El paso QR act√∫a como una iteraci√≥n que **reduce la dispersi√≥n de los autovalores en torno a cero**, y la restauraci√≥n con $+\mu_k I$ devuelve la escala original. De este modo, si $\mu_k$ es cercano a un autovalor real de $A_k$, esa componente ser√° realzada en la iteraci√≥n siguiente.

"

# ‚ïî‚ïê‚ï° 16b57aca-a13d-4774-8f58-17b795e58c01
md"
## An√°lisis num√©rico
### Implementaci√≥n
#### #1
#### #2
### Comparaci√≥n de tiempo
#### Con matrices normales y de Hessenberg
#### Diferencias de tama√±o
### Comparaci√≥n de estabilidad num√©rica
"

# ‚ïî‚ïê‚ï° Cell order:
# ‚ïü‚îÄc478a7cc-42b0-11f0-1c45-919167ce835a
# ‚ïü‚îÄ17dbbafe-fe46-4001-a1a0-5636395e89d8
# ‚ïü‚îÄ5a04ad92-ed1f-4674-858a-f36de9a335a4
# ‚ï†‚ïê06a284b8-ab2d-4b12-9153-5f8e88485e78
# ‚ï†‚ïê891fb287-b56b-46e1-bd7c-93eb7ca4c9ad
# ‚ïü‚îÄe4c54b68-e57e-4dc4-ba2e-bdc055ec67ba
# ‚ïü‚îÄ6db839bb-ea7b-486d-9ef0-19d476273b85
# ‚ï†‚ïê16b57aca-a13d-4774-8f58-17b795e58c01
