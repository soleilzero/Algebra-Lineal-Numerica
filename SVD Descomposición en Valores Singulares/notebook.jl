### A Pluto.jl notebook ###
# v0.20.3

using Markdown
using InteractiveUtils

# â•”â•â•¡ 701241c2-322f-45ca-b17d-437cf32a7ff3
using LinearAlgebra

# â•”â•â•¡ a58e9c22-51a1-11f0-0fc0-914efb421b18
md"""
# CÃ¡lculo NumÃ©rico de la SVD: Algoritmo de Golubâ€“Kahan

## Ãndice
1. Objetivos
1. IntroducciÃ³n
1. Fundamentos teÃ³ricos
3. Algoritmos
"""

# â•”â•â•¡ 11b2ebb7-626e-49c1-a294-c9b834c73bdb
md"
## Objetivos

Explicar y analizar detalladamente el algoritmo numÃ©rico para calcular la descomposiciÃ³n en valores singulares (SVD), con Ã©nfasis en el enfoque de Golubâ€“Kahan basado en la reducciÃ³n bidiagonal y la iteraciÃ³n QR implÃ­cita.

* Exponer las conexiones teÃ³ricas entre la SVD y los problemas de autovalores simÃ©tricos,
* analizar el comportamiento numÃ©rico del algoritmo clÃ¡sico frente a mÃ©todos mal condicionados,
* implementar y visualizar los pasos clave del algoritmo de Golubâ€“Kahan,
* discutir la eficiencia computacional y estrategias prÃ¡cticas.

En otro trabajo se puede aplicar la SVD computada numÃ©ricamente a un problema prÃ¡ctico.

## IntroducciÃ³n 

La descomposiciÃ³n en valores singulares (SVD) es una de las herramientas mÃ¡s poderosas y versÃ¡tiles del Ã¡lgebra lineal numÃ©rica.
No solo existe para cualquier matriz y proporciona informaciÃ³n estructural sobre ella â€”como su rango, condiciÃ³n y modos principales de acciÃ³nâ€”, sino que tambiÃ©n constituye la base de aplicaciones fundamentales en anÃ¡lisis de datos, compresiÃ³n, mÃ©todos de mÃ­nimos cuadrados, y problemas inversos.

Desde el punto de vista computacional, calcular la SVD de una matriz general $\in \mathbb{R}^{m \times n}$ de forma precisa y eficiente requiere mucho mÃ¡s que aplicar definiciones algebraicas. De hecho, formar matrices como $A^T A$ puede llevar a errores numÃ©ricos significativos. Para abordar este desafÃ­o, Golub y Kahan propusieron en 1965 un algoritmo robusto y eficiente basado en dos pasos clave:

1. **ReducciÃ³n bidiagonal de $A$** mediante transformaciones de Householder.
2. **DiagonalizaciÃ³n de la matriz bidiagonal** resultante usando una iteraciÃ³n QR adaptada, que opera de forma implÃ­cita sobre $B^T B$, sin construirla explÃ­citamente.

Este notebook estÃ¡ dedicado al estudio detallado de este algoritmo. Nuestro objetivo no es solo entender cÃ³mo se implementa, sino por quÃ© funciona, cÃ³mo se conecta con los problemas simÃ©tricos de autovalores, y quÃ© garantÃ­as numÃ©ricas ofrece.

A lo largo de este recorrido, desarrollaremos teorÃ­a, visualizaremos ejemplos pequeÃ±os y construiremos bloques de cÃ³digo que ilustran cada etapa del proceso. Este anÃ¡lisis estÃ¡ inspirado en la secciÃ³n 8.6 del libro *Matrix Computations*, de Golub y Van Loan, una referencia clÃ¡sica en el Ã¡rea.

> **Requisitos previos**: se asume familiaridad con transformaciones ortogonales (Householder, Givens), descomposiciÃ³n QR, y problemas de autovalores simÃ©tricos.

---
"

# â•”â•â•¡ 6b68d0ac-8118-44aa-940e-5a807217e562
md"
## Fundamentos teÃ³ricos de la SVD
DefiniciÃ³n y existencia

Propiedades bÃ¡sicas

SVD y matrices simÃ©tricas asociadas ?

Matriz aumentada simÃ©trica

Implicaciones algorÃ­tmicas

### ğŸŸ¢ DefiniciÃ³n y existencia de la SVD

Sea $A \in \mathbb{R}^{m \times n}$. La **descomposiciÃ³n en valores singulares (SVD)** de $A$ es una factorizaciÃ³n de la forma:

$A = U \Sigma V^T,$

donde
- las matrices $U \in \mathbb{R}^{m \times m}$ y $V \in \mathbb{R}^{n \times n}$ son ortogonales,
- la matriz $\Sigma \in \mathbb{R}^{m \times n}$ es diagonal rectangular con entradas no negativas en la diagonal:
  
$\Sigma = \begin{bmatrix}
\sigma_1 & & & \\
& & \ddots & \\
& & & \sigma_r \\
& & & & 0 \\
\end{bmatrix}, 
\quad \text{con } \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0,$

y $r = \operatorname{rank}(A)$.

Las cantidades $\sigma_i$ se llaman **valores singulares** de $A$, y son Ãºnicos. Los vectores columna de $U$ y $V$ se llaman **vectores singulares izquierdos** y **derechos**, respectivamente.

#### â— Teorema de existencia

**Toda** matriz real \( A \in \mathbb{R}^{m \times n} \), sea cuadrada, rectangular, singular o no, **admite una descomposiciÃ³n SVD** como la descrita arriba.

Esto contrasta con la factorizaciÃ³n espectral, que solo existe para matrices simÃ©tricas.

#### ğŸ§  IntuiciÃ³n geomÃ©trica

La SVD describe la acciÃ³n de la matriz $A$ como una transformaciÃ³n que:

1. **Rota** el espacio fuente (a travÃ©s de $V^T$),
2. **Escala** en direcciones ortogonales (a travÃ©s de $\Sigma$),
3. **Rota** el espacio imagen (a travÃ©s de $U$).

En otras palabras, toda matriz real lineal puede descomponerse como una secuencia de rotaciones/reflexiones y escalamientos. Esta intuiciÃ³n es presentada de manera grÃ¡fica por Visual Kernel en el CapÃ­tulo 1 Visualize Different Matrices de su colecciÃ³n SEE Matrix.


"

# â•”â•â•¡ 56ef11c7-1b71-45f7-b286-38a932ac3dc4
md"""
### ğŸŸ¢ Propiedades bÃ¡sicas de la SVD

Dada la descomposiciÃ³n $A = U \Sigma V^T$ de una matriz $A \in \mathbb{R}^{m \times n}$, se cumplen las siguientes propiedades fundamentales:

#### ğŸ”¹ Ortogonalidad de los vectores singulares

- Las **columnas de $V$** (vectores singulares derechos) son autovectores de $A^T A$ y forman una base ortonormal de $\mathbb{R}^m$.
- Las **columnas de $U$** (vectores singulares izquierdos) son autovectores de $A A^T$ y forman una base ortonormal de $\mathbb{R}^n$.


#### ğŸ”¹ InterpretaciÃ³n de los valores singulares

- Los **autovalores** de $A^T A$ y $A A^T$ son exactamente $\sigma_i^2$.
- Los valores singulares $\sigma_i$ son los **autovalores positivos** de $A^T A$ o $A A^T$:
  $A^T A = V \Sigma^2 V^T, \quad A A^T = U \Sigma^2 U^T$
- Siempre se ordenan de forma no creciente:
  $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$
- Se cumple que $\text{rank}(A) = r = \# \{ \sigma_i > 0 \}$.

#### ğŸ”¹ Normas y condiciÃ³n

- Norma 2:
  $\| A \|_2 = \sigma_1$
- Norma de Frobenius:
  $\| A \|_F = \left( \sum_{i=1}^r \sigma_i^2 \right)^{1/2}$
- NÃºmero de condiciÃ³n (si $A$ es cuadrada y de rango completo):
  $\kappa_2(A) = \frac{\sigma_1}{\sigma_r}$

#### ğŸ”¹ DescomposiciÃ³n como suma de rangos 1

La descomposiciÃ³n SVD permite representar cualquier matriz $A \in \mathbb{R}^{m \times n}$ como una **suma de matrices de rango 1**:

$A = \sum_{i=1}^r \sigma_i u_i v_i^T,$

donde $r = \operatorname{rank}(A)$, $u_i$ y $v_i$ son los vectores singulares izquierdo y derecho correspondientes al valor singular $\sigma_i$, cada tÃ©rmino $\sigma_i u_i v_i^T$ es una matriz de **rango 1**.

Esta forma revela que la SVD descompone $A$ como la suma de **modos bÃ¡sicos** ordenados por importancia (es decir, energÃ­a o norma).

##### ğŸ”¸ AproximaciÃ³n de rango bajo Ã³ptima

Una de las propiedades mÃ¡s poderosas de la SVD es que permite construir la **mejor aproximaciÃ³n de rango $k$** a $A$ en sentido de mÃ­nima distancia, tanto en norma 2 como en norma de Frobenius.

Para cualquier $k < r$, definimos:

$A_k = \sum_{i=1}^k \sigma_i u_i v_i^T,$

entonces se cumple $\| A - A_k \|_2 = \sigma_{k+1}$ y $\| A - A_k \|_F^2 = \sum_{i = k+1}^r \sigma_i^2$.

Este resultado se conoce como el **teorema de Eckartâ€“Young**, y afirma que $A_k$ es la mejor aproximaciÃ³n de $A$ por una matriz de rango $k$, en el sentido de ser la mÃ¡s cercana segÃºn dichas normas.


##### ğŸ“Œ Consecuencias prÃ¡cticas

- Los primeros tÃ©rminos de la SVD capturan la **estructura principal** de $A$.
- Los tÃ©rminos restantes pueden considerarse **ruido o detalles finos**.
- Este principio es la base de muchas tÃ©cnicas de **compresiÃ³n**, **reducciÃ³n de dimensionalidad** y **filtrado**.

Ejemplos tÃ­picos incluyen:
- Representar imÃ¡genes usando solo los primeros 10 valores singulares.
- Aproximar grandes matrices de datos con bajo rango efectivo.

"""

# â•”â•â•¡ dbb9e53d-bb68-4220-a32d-6c3ce32e274a
md"""
## Algoritmo de cÃ¡lculo de la SVD

* **OpciÃ³n 1**: $A^T A$
* * DefiniciÃ³n
* * Por quÃ© no se usa
* **OpciÃ³n 2**: Desde el punto de vista numÃ©rico, **Â¿cÃ³mo se calcula esta descomposiciÃ³n sin formar $A^T A$?**
* * Intro
* * Fundamentos teÃ³ricos
* * MÃ©todo de Golubâ€“Kahan

## ComparaciÃ³n / AnÃ¡lisis de los algoritmos
En la secciÃ³n anterior vimos que la SVD de una matriz $A \in \mathbb{R}^{m \times n}$ (con $m \geq n$) tiene la forma:

$A = U \Sigma V^T,$

donde $U$ y $V$ son ortogonales, y $\Sigma$ es una matriz diagonal rectangular con entradas no negativas.

"""

# â•”â•â•¡ 85c6df45-20ef-4db2-8703-ddf76166832e
md"""
## Algoritmos de cÃ¡lculo de la SVD

En esta secciÃ³n abordamos el problema de cÃ³mo calcular numÃ©ricamente la descomposiciÃ³n en valores singulares de una matriz $A \in \mathbb{R}^{m \times n}$. Aunque su existencia estÃ¡ garantizada teÃ³ricamente, el cÃ¡lculo efectivo de las matrices $U$, $\Sigma$ y $V$ requiere algoritmos cuidadosamente diseÃ±ados para asegurar estabilidad y eficiencia, especialmente cuando $A$ estÃ¡ mal condicionada o es de gran tamaÃ±o.

Comenzamos con el algoritmo clÃ¡sico, que se basa en la relaciÃ³n $A^T A = V \Sigma^2 V^T$ y permite obtener los valores y vectores singulares a partir de la descomposiciÃ³n espectral. Este mÃ©todo es Ãºtil para fines pedagÃ³gicos, pero es numÃ©ricamente inadecuado debido a su sensibilidad a errores de redondeo.

Luego presentaremos el mÃ©todo de Golubâ€“Kahan, que constituye la base de las implementaciones modernas de la SVD. Este enfoque evita formar $A^T A$ y combina una reducciÃ³n bidiagonal mediante transformaciones ortogonales con una versiÃ³n adaptada del algoritmo QR para matrices simÃ©tricas. Analizaremos este mÃ©todo en tres partes: una introducciÃ³n conceptual, los fundamentos teÃ³ricos que lo justifican, y una descripciÃ³n detallada del algoritmo.

Por Ãºltimo, exploraremos otras variantes relevantes como el mÃ©todo de Jacobi, los algoritmos divide-and-conquer, y algunas estrategias paralelas. Estas alternativas ofrecen ventajas particulares en tÃ©rminos de precisiÃ³n o rendimiento, y permiten comparar diferentes enfoques bajo distintas condiciones computacionales.


### Algoritmo clÃ¡sico

#### ğŸ”¹ DefiniciÃ³n

Dada una matriz $A \in \mathbb{R}^{m \times n}$, el producto $A^T A$ es una matriz simÃ©trica y semidefinida positiva de tamaÃ±o $n \times n$. Sus autovalores son no negativos, y sus autovectores son precisamente los vectores singulares derechos de $A$.

AdemÃ¡s, se cumple:

$A^T A = V \Sigma^2 V^T,$

lo que sugiere que una forma de obtener los valores singulares serÃ­a:

1. Formar $C = A^T A$,
2. Calcular sus autovalores $\lambda_i$,
3. Obtener $\sigma_i = \sqrt{\lambda_i}$,
4. Recuperar $U$ a partir de $AV = U \Sigma$.

#### ğŸ”¸ Â¿Por quÃ© no se usa este enfoque?

Aunque conceptualmente sencilla, esta estrategia es **numÃ©ricamente inestable**:

- **PÃ©rdida de precisiÃ³n:** el cÃ¡lculo de $A^T A$ **cuadra la condiciÃ³n** de $A$:
  $\kappa_2(A^T A) = \kappa_2(A)^2,$
  lo que magnifica errores cuando $A$ es mal condicionada.

- **Cancelaciones destructivas:** en aritmÃ©tica de punto flotante, calcular $A^T A$ puede eliminar informaciÃ³n valiosa contenida en $A$.

- **No preserva la estructura original:** operaciones como la bidiagonalizaciÃ³n sÃ­ preservan mejor las propiedades numÃ©ricas de $A$.

Por estas razones, los algoritmos modernos evitan formar explÃ­citamente $A^T A$, y utilizan en cambio mÃ©todos mÃ¡s estables, como el de **Golubâ€“Kahan**.


"""

# â•”â•â•¡ 74470dd1-eb43-4039-9d93-6bdf32768466
md"""
### MÃ©todo de Golubâ€“Kahan

El algoritmo de Golubâ€“Kahan para calcular la SVD de una matriz $A \in \mathbb{R}^{m \times n}$ (con $m \geq n$) consta de dos fases principales:

#### Paso a paso
##### ğŸ”¹ Paso 1: ReducciÃ³n bidiagonal

El primer paso transforma la matriz original $A$ en una matriz **bidiagonal** $B$, es decir, una matriz con ceros fuera de la diagonal y la superdiagonal:

$B = U_1^T A V_1$

Este paso se logra aplicando **transformaciones ortogonales de Householder** por la izquierda y la derecha, que eliminan progresivamente los elementos debajo de la diagonal y fuera de la banda bidiagonal.

La estructura resultante tiene la forma:

$B = \begin{bmatrix}
d_1 & f_1 &        &        & \\
    & d_2 & f_2    &        & \\
    &     & \ddots & \ddots & \\
    &     &        & d_{n-1} & f_{n-1} \\
    &     &        &        & d_n
\end{bmatrix}$

Donde $d_i$ son los elementos diagonales y $f_i$ los elementos de la superdiagonal.

Este paso convierte el problema general en uno estructurado, sin alterar los valores singulares, y ademÃ¡s **preserva la estabilidad numÃ©rica**.

##### ğŸ”¹ Paso 2: DiagonalizaciÃ³n bidiagonal (iteraciÃ³n QR simÃ©trica)

Una vez reducida $A$ a bidiagonal, el siguiente objetivo es transformar $B$ en una matriz **diagonal**, es decir, eliminar su superdiagonal sin perturbar la estructura.

Para esto se emplea una versiÃ³n adaptada del **algoritmo QR simÃ©trico con desplazamiento**. En lugar de formar $B^T B$ (que serÃ­a tridiagonal), se trabaja directamente sobre $B$ aplicando secuencias de **rotaciones de Givens** que eliminan progresivamente los elementos $f_i$.

Cada iteraciÃ³n consta de:

1. CÃ¡lculo de un **desplazamiento (shift)** aproximado del valor singular, basado en el extremo de la banda bidiagonal.
2. AplicaciÃ³n de rotaciones alternadas por la derecha e izquierda (tipo QR simÃ©trico implÃ­cito) para "empujar" el elemento fuera de banda hacia la esquina inferior.
3. EliminaciÃ³n (o reducciÃ³n por deflaciÃ³n) cuando algÃºn $f_i$ es suficientemente pequeÃ±o en magnitud.

Este proceso se repite hasta que toda la superdiagonal de $B$ se anula, y la matriz resultante se convierte en la matriz $\Sigma$ de la SVD. Las rotaciones aplicadas se acumulan en matrices ortogonales $U_2$ y $V_2$.


##### ğŸ”¹ Paso 3: ComposiciÃ³n final

Finalmente, la SVD completa de $A$ se reconstruye a partir de las transformaciones aplicadas:

$A = U \Sigma V^T, \quad \text{donde } U = U_1 U_2,\quad V = V_1 V_2$

En esta descomposiciÃ³n:
- la matriz $\Sigma$ es una matriz diagonal con los valores singulares ordenados,
- las matrices $U$ y $V$ son ortogonales, y
- el proceso es **estable**, ya que todas las operaciones se realizan con transformaciones ortogonales.

"""

# â•”â•â•¡ 247c0f32-0141-424f-9e8d-6dda19a521db
md"""
#### ğŸ¤– ImplementaciÃ³n

Supuestos:

* La matriz $B$ es **bidiagonal superior**: solo tiene elementos en la diagonal $d$ y superdiagonal $f$.
* No se acumulan rotaciones (esto puede aÃ±adirse).
* El paso trabaja **in-place** sobre los vectores `d` y `f`.

Notas:

* Esta implementaciÃ³n **modifica `d` y `f` in-place**.
* No acumula las transformaciones $U$ y $V$, pero eso puede aÃ±adirse si se desea formar explÃ­citamente las matrices singulares.
* El desplazamiento de Wilkinson garantiza una buena convergencia, especialmente cerca del valor singular mÃ¡s pequeÃ±o.

"""

# â•”â•â•¡ 3d76d8e3-fa4a-4e88-81cd-7489cdb146d7
md"##### ğŸ‘¾ Desplazamiento de Wilkinson"

# â•”â•â•¡ d5e98c9e-92ee-454d-9de8-7c5f38e89c68
"""
Calcula el shift de Wilkinson (mu) a partir de los tres elementos finales de B bidiagonal.

Inputs:
- dm: elemento d_{n-1} (penÃºltimo de la diagonal)
- fm: elemento f_{n-1} (Ãºltimo de la superdiagonal)
- dn: elemento d_n (Ãºltimo de la diagonal)

Output:
- mu: shift de Wilkinson
"""
function wilkinson_shift(dm::Float64, fm::Float64, dn::Float64)
    tmm = dm^2 + fm^2
    tmn = fm * dn
    tnn = dn^2

    Î´ = (tmm - tnn) / 2
    denom = abs(Î´) + sqrt(Î´^2 + tmn^2)

    if denom == 0
        return tnn
    else
        return tnn - sign(Î´) * tmn^2 / denom
    end
end


# â•”â•â•¡ 8ca30862-249a-444d-8a79-702ec8732935
"""
Compara `wilkinson_shift` con el verdadero autovalor de una matriz 2x2 para validar.
"""
function validate_shift(dm, fm, dn)
    T = [
        dm^2 + fm^2    fm * dn
        fm * dn        dn^2
    ]
    Î» = eigen(T).values
    Î¼_ref = Î»[argmin(abs.(Î» .- dn^2))]  # autovalor mÃ¡s cercano a t_nn
    Î¼_custom = wilkinson_shift(dm, fm, dn)
    
    println("Î¼ (eigen 2Ã—2)     = $Î¼_ref")
    println("Î¼ (Wilkinson calc)= $Î¼_custom")
    println("Error             = $(abs(Î¼_custom - Î¼_ref))")
end


# â•”â•â•¡ e9b5e9ee-83ae-41e0-b5cd-d7772d95d27b
validate_shift(3.0, 0.01, 4.0)

# â•”â•â•¡ 906732db-6348-495d-909c-017bbc036659
validate_shift(1.0, 0.5, 1.2)

# â•”â•â•¡ 1740f53a-0e59-4fa7-b63e-e74c0b8bb484
md"##### ğŸ‘¾ Rotaciones de Givens"

# â•”â•â•¡ 7902449e-e1ca-4dd7-aef1-96d32e051f40
"""
Devuelve (c, s, r) tal que [c s; -s c] * [y; z] = [r; 0]
"""
function givens_rotation(y::Float64, z::Float64)
    if z == 0
        return (1.0, 0.0, y)
    elseif y == 0
        return (0.0, 1.0, z)
    else
        r = hypot(y, z)
        return (y / r, z / r, r)
    end
end


# â•”â•â•¡ d79631a6-4494-49bd-a9e5-8b100de0272d
"""
Aplica rotaciÃ³n de Givens por la derecha sobre d[k] y f[k]
"""
function apply_right_rotation!(d, f, k::Int, c::Float64, s::Float64)
    d_k  = d[k]
    f_k  = f[k]
    d[k] =  c * d_k + s * f_k
    f[k] = -s * d_k + c * f_k
end


# â•”â•â•¡ 2edc23a7-b981-4416-8516-54084054bd74
"""
Aplica rotaciÃ³n de Givens por la izquierda sobre d[k] y d[k+1]
"""
function apply_left_rotation!(d, k::Int, c::Float64, s::Float64)
    d_k  = d[k]
    d_k1 = d[k+1]
    d[k]   =  c * d_k + s * d_k1
    d[k+1] = -s * d_k + c * d_k1
end


# â•”â•â•¡ c0d5d51b-09c8-407a-8169-bd8937545954
md"##### ğŸ‘¾ Paso de Golub-Kahan"

# â•”â•â•¡ c2105a86-9dda-4ccb-a9df-cba3c10b6161
"""
Construye una matriz bidiagonal superior B âˆˆ â„^{nÃ—n} a partir de:

- d: diagonal de B, longitud n
- f: superdiagonal de B, longitud n - 1

Retorna: B :: Matrix{Float64}, una matriz nÃ—n con estructura bidiagonal superior.
"""
function build_bidiagonal(d, f)
    n = length(d)
    @assert length(f) == n - 1 "La superdiagonal f debe tener longitud n - 1"

    B = zeros(Float64, n, n)

    for i in 1:n
        B[i, i] = d[i]
        if i < n
            B[i, i+1] = f[i]
        end
    end

    return B
end


# â•”â•â•¡ a1c28c45-10de-460c-909f-ab724f2afd45
"""
Valida un paso de Golubâ€“Kahan aplicado a una matriz bidiagonal B.

- step_func: funciÃ³n que modifica B in-place (como golub_kahan_svd_step_matrix!)
- B: matriz bidiagonal cuadrada (con solo diagonal y superdiagonal)

Opcional:
- verbose = true: imprime detalles
- atol: tolerancia para comparar autovalores

Retorna: true si todo pasa, false si falla alguna prueba.
"""
function validate_golub_kahan_step(step_func::Function, B::Matrix{Float64};
                                   atol=1e-10, verbose=true)
    n = size(B, 1)
    @assert size(B, 2) == n "B debe ser cuadrada"

    # --- Copiar datos originales ---
    B_before = copy(B)
    T_before = B_before' * B_before
    Î»_before = sort(eigvals(Symmetric(T_before)))

    # --- Aplicar paso QR bidiagonal ---
    step_func(B)

    # --- Verificar estructura bidiagonal ---
    is_bidiagonal = all(i == j || j == i+1 || B[i, j] == 0.0 for i in 1:n, j in 1:n)

    # --- Comparar autovalores ---
    T_after = B' * B
    Î»_after = sort(eigvals(Symmetric(T_after)))
    Î»_diff = norm(Î»_before - Î»_after, Inf)

    # --- Imprimir si se desea ---
    if verbose
        println("âœ” Estructura bidiagonal: ", is_bidiagonal)
        println("âœ” Autovalores antes:  ", round.(Î»_before, digits=8))
        println("âœ” Autovalores despuÃ©s:", round.(Î»_after, digits=8))
        println("Î”Î» âˆ-norm: ", Î»_diff)
    end

    return is_bidiagonal && Î»_diff â‰¤ atol
end


# â•”â•â•¡ fd6ebedb-0a06-4675-adb2-7076f96fe25b
md"###### ğŸ‘¾ VersiÃ³n implÃ­cita"

# â•”â•â•¡ 9a929fc1-a2b2-474a-9deb-4d8a6bc6d7d1
function golub_kahan_svd_step!(d::Vector{Float64}, f::Vector{Float64})
    n = length(d)
    @assert length(f) == n - 1

    if n < 2
        return d, f
    end

    # --- Paso 1: calcular el shift de Wilkinson ---
    Î¼ = wilkinson_shift(d[end-1], f[end], d[end])

    # --- Paso 2: inicializar el bulge ---
    x = d[1]^2 - Î¼
    z = d[1] * f[1]

    for k in 1:n-1
        # --- RotaciÃ³n por la derecha: columnas k, k+1 ---
        c, s, _ = givens_rotation(x, z)

        d_k   = d[k]
        f_k   = f[k]
        d_kp1 = d[k+1]

        # Actualizar d[k] y f[k]
        d[k] =  c * d_k + s * f_k
        f[k] = -s * d_k + c * f_k

        # Crear bulge en d[k+1]
        x = d[k]
        z = d_kp1

        # --- RotaciÃ³n por la izquierda: filas k, k+1 ---
        c, s, _ = givens_rotation(x, z)

        d[k]     =  c * x + s * z
        d[k+1]   = -s * x + c * z

        # Actualizar f[k] y f[k+1] si no es el Ãºltimo paso
        if k < n - 1
            f_kp1 = f[k+1]
            x = f[k]
            z = f_kp1
            c, s, _ = givens_rotation(x, z)
            f[k]   =  c * x + s * z
            f[k+1] = 0.0  # bulge eliminado
        end
    end

    return d, f
end


# â•”â•â•¡ 9cd5e17f-721a-4b99-bd80-db95a1ef99f0
begin
	golub_kahan_svd_step!([4.0, 3.0, 2.0] , [1.0, 0.5])
	golub_kahan_svd_step!([1.0, 2.0, 3.0, 4.0] , [1.0, 1.0, 0.01])
end

# â•”â•â•¡ 8065ba2c-c0ce-4cc8-87d2-e98fad2868a3
begin
	d = [3.0, 2.0, 1.0]
	f = [0.5, 0.3]
	
	validate_golub_kahan_step(d, f)
	
end

# â•”â•â•¡ 004da5d0-d039-4f0c-890b-16cdf36d21f9
md"###### ğŸ‘¾ VersiÃ³n explÃ­cita"

# â•”â•â•¡ adba5d98-0080-4d56-81ca-897d8a97eb39
"""
Aplica un paso QR bidiagonal de Golubâ€“Kahan a una matriz bidiagonal superior B âˆˆ â„^{nÃ—n}.

- B debe tener solo elementos en la diagonal y superdiagonal.
- La estructura bidiagonal se preserva.
- La transformaciÃ³n es ortogonal y mantiene aproximadamente los autovalores de Báµ—B.

Modifica B in-place.
"""
function golub_kahan_svd_step_matrix!(B::Matrix{Float64})
    n = size(B, 1)
    @assert size(B, 2) == n

    # Calcular el shift de Wilkinson
    d = diag(B)
    f = [B[i, i+1] for i in 1:n-1]
    Î¼ = wilkinson_shift(d[end-1], f[end], d[end])

    # Inicializar el primer vector para la rotaciÃ³n por la derecha
    y = d[1]^2 - Î¼
    z = d[1] * B[1, 2]

    for k in 1:n-1
        # -------- RotaciÃ³n por la derecha (columnas k, k+1) --------
        c, s, _ = givens_rotation(y, z)

        # Solo columnas k y k+1 de filas k y siguientes (preservar estructura)
        for i in k:n
            t1 = B[i, k]
            t2 = B[i, k+1]
            B[i, k]   =  c * t1 + s * t2
            B[i, k+1] = -s * t1 + c * t2
        end

        # -------- RotaciÃ³n por la izquierda (filas k, k+1) --------
        y = B[k, k]
        z = B[k+1, k]
        c, s, _ = givens_rotation(y, z)

        # Solo filas k y k+1 de columnas k y siguientes
        for j in k:n
            t1 = B[k, j]
            t2 = B[k+1, j]
            B[k, j]   =  c * t1 + s * t2
            B[k+1, j] = -s * t1 + c * t2
        end

        # Preparar vector (y, z) para la prÃ³xima rotaciÃ³n por la derecha
        if k < n - 1
            y = B[k, k+1]
            z = B[k, k+2]
        end
    end

    # Limpiar numÃ©ricamente la matriz fuera de la banda bidiagonal
    for i in 1:n
        for j in 1:n
            if j â‰  i && j â‰  i+1
                B[i, j] = 0.0
            end
        end
    end

    return B
end


# â•”â•â•¡ 2607c879-d632-4b80-8523-123221a01303
begin
	golub_kahan_svd_step_matrix!(build_bidiagonal([4.0, 3.0, 2.0] , [1.0, 0.5]))
	golub_kahan_svd_step_matrix!(build_bidiagonal([1.0, 2.0, 3.0, 4.0] , [1.0, 1.0, 0.01]))
end

# â•”â•â•¡ 8ec72b94-d090-4c76-bde4-70787a67461a
begin
	B = build_bidiagonal([1.0, 2.0, 3.0, 4.0], [1.0, 1.0, 0.01])
	
	validate_golub_kahan_step(golub_kahan_svd_step_matrix!, B)
end

# â•”â•â•¡ 5bd0c109-17c9-4e93-bdfb-e8faf660f708
md"##### ğŸ‘¾ Algoritmo de Golub-Kahan"

# â•”â•â•¡ b2928cde-9e9b-4d28-8a0a-45bae8e8f4ef
"""
Aplica el algoritmo completo de Golubâ€“Kahan para diagonalizar una matriz bidiagonal.

Input:
- d: Vector{Float64} con la diagonal de la matriz bidiagonal B
- f: Vector{Float64} con la superdiagonal de B
- Ïµ: Tolerancia (por defecto, 100Ã—eps(Float64))

Output:
- d: Diagonal actualizada (valores singulares)
- f: Superdiagonal (debe terminar cercana a cero)
"""
function golub_kahan_svd!(d::Vector{Float64}, f::Vector{Float64}; Ïµ = 100 * eps(Float64))
    n = length(d)
    @assert length(f) == n - 1

    # Ciclo externo: repetir hasta que todos los f[i] sean pequeÃ±os
    while true
		display(f)
		display(d)
        # Paso 1: forzar ceros pequeÃ±os en f por deflaciÃ³n
        for i in 1:n-1
            tol = Ïµ * (abs(d[i]) + abs(d[i+1]))
            if abs(f[i]) â‰¤ tol
                f[i] = 0.0
            end
        end

        # Paso 2: buscar submatrices no diagonales
        # Buscar el mayor Ã­ndice q tal que f[q] â‰  0
        q = n
        while q > 1 && f[q-1] == 0.0
            q -= 1
        end

        # Si ya no queda banda activa, terminar
        if q == 1
            break
        end

        # Buscar el menor p tal que f[p] â‰  0
        p = q - 1
        while p > 1 && f[p-1] â‰  0.0
            p -= 1
        end

        # Verificar si hay ceros en la diagonal de B_{p:q}
        zero_on_diag = false
        for i in p:q
            if abs(d[i]) â‰¤ Ïµ * maximum(abs.(d))
                zero_on_diag = true
                break
            end
        end

        if zero_on_diag
            # Si hay ceros en la diagonal, hacer cero la fila correspondiente
            for i in p:q-1
                if abs(d[i]) â‰¤ Ïµ * maximum(abs.(d))
                    f[i] = 0.0
                end
            end
        elseif q - p + 1 >= 2
            # Aplicar un paso QR bidiagonal al bloque activo d[p:q], f[p:q-1]
            dblock = view(d, p:q)
            fblock = view(f, p:q-1)
            golub_kahan_svd_step_matrix!(build_bidiagonal(dblock, fblock))
        end
    end

    return d
end


# â•”â•â•¡ 7c84c4d6-27fc-4c78-9002-eeb6b8780272
#golub_kahan_svd!([4.0, 3.0, 2.0], [1.0, 0.5])
	
# Resultado: `d` contiene los valores singulares aproximados de la matriz bidiagonal

# â•”â•â•¡ 00ca8f64-6937-45ee-8970-d1c2bf49fd59
md"
To Do:
- [ ] Add other implementation (either classical or Jacobi)
- [ ] Have a working Golub

"

# â•”â•â•¡ 00000000-0000-0000-0000-000000000001
PLUTO_PROJECT_TOML_CONTENTS = """
[deps]
LinearAlgebra = "37e2e46d-f89d-539d-b4ee-838fcccc9c8e"
"""

# â•”â•â•¡ 00000000-0000-0000-0000-000000000002
PLUTO_MANIFEST_TOML_CONTENTS = """
# This file is machine-generated - editing it directly is not advised

julia_version = "1.11.1"
manifest_format = "2.0"
project_hash = "ac1187e548c6ab173ac57d4e72da1620216bce54"

[[deps.Artifacts]]
uuid = "56f22d72-fd6d-98f1-02f0-08ddc0907c33"
version = "1.11.0"

[[deps.CompilerSupportLibraries_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "e66e0078-7015-5450-92f7-15fbd957f2ae"
version = "1.1.1+0"

[[deps.Libdl]]
uuid = "8f399da3-3557-5675-b5ff-fb832c97cbdb"
version = "1.11.0"

[[deps.LinearAlgebra]]
deps = ["Libdl", "OpenBLAS_jll", "libblastrampoline_jll"]
uuid = "37e2e46d-f89d-539d-b4ee-838fcccc9c8e"
version = "1.11.0"

[[deps.OpenBLAS_jll]]
deps = ["Artifacts", "CompilerSupportLibraries_jll", "Libdl"]
uuid = "4536629a-c528-5b80-bd46-f80d51c5b363"
version = "0.3.27+1"

[[deps.libblastrampoline_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "8e850b90-86db-534c-a0d3-1478176c7d93"
version = "5.11.0+0"
"""

# â•”â•â•¡ Cell order:
# â•Ÿâ”€a58e9c22-51a1-11f0-0fc0-914efb421b18
# â• â•701241c2-322f-45ca-b17d-437cf32a7ff3
# â•Ÿâ”€11b2ebb7-626e-49c1-a294-c9b834c73bdb
# â•Ÿâ”€6b68d0ac-8118-44aa-940e-5a807217e562
# â•Ÿâ”€56ef11c7-1b71-45f7-b286-38a932ac3dc4
# â•Ÿâ”€dbb9e53d-bb68-4220-a32d-6c3ce32e274a
# â•Ÿâ”€85c6df45-20ef-4db2-8703-ddf76166832e
# â•Ÿâ”€74470dd1-eb43-4039-9d93-6bdf32768466
# â•Ÿâ”€247c0f32-0141-424f-9e8d-6dda19a521db
# â•Ÿâ”€3d76d8e3-fa4a-4e88-81cd-7489cdb146d7
# â•Ÿâ”€d5e98c9e-92ee-454d-9de8-7c5f38e89c68
# â•Ÿâ”€8ca30862-249a-444d-8a79-702ec8732935
# â• â•e9b5e9ee-83ae-41e0-b5cd-d7772d95d27b
# â• â•906732db-6348-495d-909c-017bbc036659
# â•Ÿâ”€1740f53a-0e59-4fa7-b63e-e74c0b8bb484
# â•Ÿâ”€7902449e-e1ca-4dd7-aef1-96d32e051f40
# â•Ÿâ”€d79631a6-4494-49bd-a9e5-8b100de0272d
# â•Ÿâ”€2edc23a7-b981-4416-8516-54084054bd74
# â•Ÿâ”€c0d5d51b-09c8-407a-8169-bd8937545954
# â•Ÿâ”€c2105a86-9dda-4ccb-a9df-cba3c10b6161
# â•Ÿâ”€a1c28c45-10de-460c-909f-ab724f2afd45
# â•Ÿâ”€fd6ebedb-0a06-4675-adb2-7076f96fe25b
# â•Ÿâ”€9a929fc1-a2b2-474a-9deb-4d8a6bc6d7d1
# â• â•9cd5e17f-721a-4b99-bd80-db95a1ef99f0
# â• â•8065ba2c-c0ce-4cc8-87d2-e98fad2868a3
# â•Ÿâ”€004da5d0-d039-4f0c-890b-16cdf36d21f9
# â• â•adba5d98-0080-4d56-81ca-897d8a97eb39
# â• â•2607c879-d632-4b80-8523-123221a01303
# â• â•8ec72b94-d090-4c76-bde4-70787a67461a
# â•Ÿâ”€5bd0c109-17c9-4e93-bdfb-e8faf660f708
# â• â•b2928cde-9e9b-4d28-8a0a-45bae8e8f4ef
# â• â•7c84c4d6-27fc-4c78-9002-eeb6b8780272
# â• â•00ca8f64-6937-45ee-8970-d1c2bf49fd59
# â•Ÿâ”€00000000-0000-0000-0000-000000000001
# â•Ÿâ”€00000000-0000-0000-0000-000000000002
